{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Health Prediction ML Pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ML libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve, classification_report, f1_score\n",
        ")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Models\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    ExtraTreesClassifier,\n",
        "    HistGradientBoostingClassifier\n",
        ")\n",
        "import xgboost as xgb\n",
        "\n",
        "# Configuration\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.3\n",
        "DATA_PATH = \"data/\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_clean_data(train_path, test_path):\n",
        "    train = pd.read_csv(train_path)\n",
        "    test = pd.read_csv(test_path)\n",
        "    cols_all_nan_train = train.columns[train.isna().mean() == 1.0]\n",
        "    cols_all_nan_test = test.columns[test.isna().mean() == 1.0]\n",
        "    train = train.drop(columns=cols_all_nan_train)\n",
        "    test = test.drop(columns=cols_all_nan_test)\n",
        "    print(f\"Train: {train.shape} | Test: {test.shape}\")\n",
        "    print(f\"Removed {len(cols_all_nan_train)} columns\")\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def prepare_data(df, target_col='TARGET'):\n",
        "    y = df[target_col]\n",
        "    if y.dtype == bool or set(pd.unique(y)) <= {True, False}:\n",
        "        y = y.astype(int)\n",
        "    X = df.drop(columns=[target_col])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def create_preprocessor(X, encoding='ordinal'):\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "    \n",
        "    if encoding == 'ordinal':\n",
        "        cat_transformer = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
        "        ])\n",
        "    else:\n",
        "        cat_transformer = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "        ])\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', SimpleImputer(strategy='median'), num_cols),\n",
        "            ('cat', cat_transformer, cat_cols)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    print(f\"Features: {len(num_cols)} numerical, {len(cat_cols)} categorical\")\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def find_best_threshold(y_true, y_proba):\n",
        "    prec, rec, thresholds = precision_recall_curve(y_true, y_proba)\n",
        "    f1_scores = 2 * (prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
        "    best_idx = int(np.nanargmax(f1_scores))\n",
        "    return float(thresholds[best_idx]), float(f1_scores[best_idx])\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_val, y_val, model_name='Model'):\n",
        "    y_proba = model.predict_proba(X_val)[:, 1]\n",
        "    best_thr, best_f1 = find_best_threshold(y_val, y_proba)\n",
        "    y_pred = (y_proba >= best_thr).astype(int)\n",
        "    print(f\"\\n{model_name}: threshold={best_thr:.4f}, F1={best_f1:.4f}\")\n",
        "    print(classification_report(y_val, y_pred, digits=3))\n",
        "    return best_thr, best_f1\n",
        "\n",
        "\n",
        "def export_predictions(model, X_test, test_df, threshold, filename):\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = (y_proba >= threshold).astype(int)\n",
        "    id_col = 'ID' if 'ID' in test_df.columns else test_df.columns[-1]\n",
        "    output = pd.DataFrame({id_col: test_df[id_col], 'pred': y_pred})\n",
        "    output.to_csv(filename, index=False)\n",
        "    print(f\"Predictions saved: {filename}\")\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (225000, 322) | Test: (75000, 321)\n",
            "Removed 3 columns\n",
            "Features: 322, Samples: 225000\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df = load_and_clean_data(f\"{DATA_PATH}train.csv\", f\"{DATA_PATH}test.csv\")\n",
        "print(f\"Features: {train_df.shape[1]}, Samples: {train_df.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target: 0=204,861, 1=20,139, ratio=8.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\jules\\AppData\\Local\\Temp\\ipykernel_36224\\2758776188.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  print(f\"Target: 0={target_counts[0]:,}, 1={target_counts[1]:,}, ratio={target_counts[1]/target_counts.sum():.2%}\")\n"
          ]
        }
      ],
      "source": [
        "target_col = 'TARGET'\n",
        "target_counts = train_df[target_col].value_counts()\n",
        "print(f\"Target: 0={target_counts[0]:,}, 1={target_counts[1]:,}, ratio={target_counts[1]/target_counts.sum():.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 correlated features:\n",
            "_AGE80      0.231371\n",
            "_AGEG5YR    0.222521\n",
            "GENHLTH     0.222041\n",
            "_AGE_G      0.214506\n",
            "COLGSEX1    0.206725\n",
            "EMPLOY1     0.203364\n",
            "_HCVU652    0.194126\n",
            "_AGE65YR    0.184403\n",
            "_DRDXAR2    0.169732\n",
            "_PACKYRS    0.168462\n",
            "Name: TARGET, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "df_num = train_df.apply(pd.to_numeric, errors='coerce')\n",
        "corr_with_target = df_num.corr()[target_col].drop(target_col).abs().sort_values(ascending=False)\n",
        "print(\"Top 10 correlated features:\")\n",
        "print(corr_with_target.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split: 157,500 train, 67,500 validation\n"
          ]
        }
      ],
      "source": [
        "X_all, y_all = prepare_data(train_df, target_col)\n",
        "X_test_final = test_df.drop(columns=[target_col], errors='ignore')\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size=TEST_SIZE, stratify=y_all, random_state=RANDOM_STATE)\n",
        "print(f\"Split: {X_train.shape[0]:,} train, {X_val.shape[0]:,} validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_results = {}\n",
        "classes = np.array([0, 1])\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
        "sample_weights = y_train.map(dict(zip(classes, class_weights))).values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Random Forest...\n",
            "Features: 321 numerical, 0 categorical\n",
            "\n",
            "RandomForest: threshold=0.6090, F1=0.3932\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.950     0.890     0.919     61458\n",
            "           1      0.316     0.520     0.393      6042\n",
            "\n",
            "    accuracy                          0.856     67500\n",
            "   macro avg      0.633     0.705     0.656     67500\n",
            "weighted avg      0.893     0.856     0.872     67500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Random Forest...\")\n",
        "preprocessor_rf = create_preprocessor(X_train, encoding='onehot')\n",
        "rf_model = Pipeline([\n",
        "    ('preprocessor', preprocessor_rf),\n",
        "    ('classifier', RandomForestClassifier(\n",
        "        n_estimators=300, max_depth=12, min_samples_leaf=2, max_features='sqrt',\n",
        "        class_weight='balanced_subsample', n_jobs=-1, random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_threshold, rf_f1 = evaluate_model(rf_model, X_val, y_val, model_name='RandomForest')\n",
        "models_results['RandomForest'] = (rf_model, rf_threshold, rf_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Extra Trees...\n",
            "Features: 321 numerical, 0 categorical\n",
            "\n",
            "ExtraTrees: threshold=0.5203, F1=0.3609\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.952     0.847     0.896     61458\n",
            "           1      0.265     0.564     0.361      6042\n",
            "\n",
            "    accuracy                          0.821     67500\n",
            "   macro avg      0.609     0.705     0.628     67500\n",
            "weighted avg      0.890     0.821     0.848     67500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Extra Trees...\")\n",
        "preprocessor_et = create_preprocessor(X_train, encoding='ordinal')\n",
        "et_model = Pipeline([\n",
        "    ('preprocessor', preprocessor_et),\n",
        "    ('classifier', ExtraTreesClassifier(\n",
        "        n_estimators=400, max_depth=18, min_samples_leaf=2, max_features='sqrt',\n",
        "        class_weight='balanced_subsample', bootstrap=True, max_samples=0.8,\n",
        "        n_jobs=-1, random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "et_model.fit(X_train, y_train)\n",
        "et_threshold, et_f1 = evaluate_model(et_model, X_val, y_val, model_name='ExtraTrees')\n",
        "models_results['ExtraTrees'] = (et_model, et_threshold, et_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Histogram Gradient Boosting...\n",
            "Features: 321 numerical, 0 categorical\n",
            "\n",
            "HistGradientBoosting: threshold=0.7343, F1=0.4177\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.950     0.905     0.927     61458\n",
            "           1      0.350     0.519     0.418      6042\n",
            "\n",
            "    accuracy                          0.871     67500\n",
            "   macro avg      0.650     0.712     0.672     67500\n",
            "weighted avg      0.897     0.871     0.882     67500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training Histogram Gradient Boosting...\")\n",
        "preprocessor_hgb = create_preprocessor(X_train, encoding='ordinal')\n",
        "hgb_model = Pipeline([\n",
        "    ('preprocessor', preprocessor_hgb),\n",
        "    ('classifier', HistGradientBoostingClassifier(\n",
        "        max_iter=300, learning_rate=0.06, max_depth=6, min_samples_leaf=20,\n",
        "        l2_regularization=1.0, early_stopping=True, validation_fraction=0.1,\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "hgb_model.fit(X_train, y_train, classifier__sample_weight=sample_weights)\n",
        "hgb_threshold, hgb_f1 = evaluate_model(hgb_model, X_val, y_val, model_name='HistGradientBoosting')\n",
        "models_results['HistGradientBoosting'] = (hgb_model, hgb_threshold, hgb_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training XGBoost...\n",
            "Features: 321 numerical, 0 categorical\n",
            "XGBoost: threshold=0.7276, F1=0.4159\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.949     0.910     0.929     61458\n",
            "           1      0.355     0.502     0.416      6042\n",
            "\n",
            "    accuracy                          0.874     67500\n",
            "   macro avg      0.652     0.706     0.673     67500\n",
            "weighted avg      0.896     0.874     0.883     67500\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Training XGBoost...\")\n",
        "preprocessor_xgb = create_preprocessor(X_train, encoding='ordinal')\n",
        "X_train_xgb = preprocessor_xgb.fit_transform(X_train)\n",
        "X_val_xgb = preprocessor_xgb.transform(X_val)\n",
        "\n",
        "scale_pos_weight = (y_train == 0).sum() / max((y_train == 1).sum(), 1)\n",
        "dtrain = xgb.DMatrix(X_train_xgb, label=y_train)\n",
        "dval = xgb.DMatrix(X_val_xgb, label=y_val)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic', 'eval_metric': 'aucpr',\n",
        "    'eta': 0.03, 'max_depth': 6, 'subsample': 0.8, 'colsample_bytree': 0.8,\n",
        "    'scale_pos_weight': scale_pos_weight, 'seed': RANDOM_STATE\n",
        "}\n",
        "\n",
        "xgb_model = xgb.train(params, dtrain, num_boost_round=2000, evals=[(dtrain, 'train'), (dval, 'eval')], \n",
        "                      early_stopping_rounds=100, verbose_eval=False)\n",
        "\n",
        "y_proba_xgb = xgb_model.predict(dval)\n",
        "xgb_threshold, xgb_f1 = find_best_threshold(y_val, y_proba_xgb)\n",
        "y_pred_xgb = (y_proba_xgb >= xgb_threshold).astype(int)\n",
        "print(f\"XGBoost: threshold={xgb_threshold:.4f}, F1={xgb_f1:.4f}\")\n",
        "print(classification_report(y_val, y_pred_xgb, digits=3))\n",
        "\n",
        "models_results['XGBoost'] = (xgb_model, xgb_threshold, xgb_f1, preprocessor_xgb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best model: HistGradientBoosting (F1=0.4177, threshold=0.7343)\n",
            "\n",
            "All results:\n",
            "  RandomForest         F1=0.3932  threshold=0.6090\n",
            "  ExtraTrees           F1=0.3609  threshold=0.5203\n",
            "  HistGradientBoosting F1=0.4177  threshold=0.7343\n",
            "  XGBoost              F1=0.4159  threshold=0.7276\n"
          ]
        }
      ],
      "source": [
        "best_model_name = max(models_results.keys(), key=lambda k: models_results[k][2])\n",
        "best_model_info = models_results[best_model_name]\n",
        "\n",
        "print(f\"\\nBest model: {best_model_name} (F1={best_model_info[2]:.4f}, threshold={best_model_info[1]:.4f})\")\n",
        "print(\"\\nAll results:\")\n",
        "for name, (_, threshold, f1, *_) in models_results.items():\n",
        "    print(f\"  {name:20} F1={f1:.4f}  threshold={threshold:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved: predictions_final.csv\n",
            "Saved predictions_final.csv using HistGradientBoosting\n",
            "       ID  pred\n",
            "0  225000     0\n",
            "1  225001     0\n",
            "2  225002     0\n",
            "3  225003     0\n",
            "4  225004     0\n",
            "5  225005     0\n",
            "6  225006     0\n",
            "7  225007     0\n",
            "8  225008     0\n",
            "9  225009     1\n"
          ]
        }
      ],
      "source": [
        "best_model = best_model_info[0]\n",
        "best_threshold = best_model_info[1]\n",
        "\n",
        "if best_model_name == 'XGBoost':\n",
        "    preprocessor = best_model_info[3]\n",
        "    X_test_transformed = preprocessor.transform(X_test_final)\n",
        "    dtest = xgb.DMatrix(X_test_transformed)\n",
        "    y_test_pred = (best_model.predict(dtest) >= best_threshold).astype(int)\n",
        "    id_col = 'ID' if 'ID' in test_df.columns else test_df.columns[-1]\n",
        "    output = pd.DataFrame({id_col: test_df[id_col], 'pred': y_test_pred})\n",
        "    output.to_csv('predictions_final.csv', index=False)\n",
        "else:\n",
        "    output = export_predictions(best_model, X_test_final, test_df, best_threshold, 'predictions_final.csv')\n",
        "\n",
        "print(f\"Saved predictions_final.csv using {best_model_name}\")\n",
        "print(output.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "Minimalistic pipeline for health prediction:\n",
        "- Data loading and preprocessing\n",
        "- Model training (RandomForest, ExtraTrees, HistGradientBoosting, XGBoost)\n",
        "- Threshold optimization for F1 score\n",
        "- Model comparison and prediction export\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
